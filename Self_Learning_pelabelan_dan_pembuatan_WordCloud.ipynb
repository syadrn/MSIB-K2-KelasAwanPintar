{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "V5VHItDM1Piu",
   "metadata": {
    "id": "V5VHItDM1Piu"
   },
   "source": [
    "# **Latihan Data Exploration NLP**\n",
    "\n",
    "NLP adalah cabang dari Artificial Intelligence yang berhubungan dengan interaksi antara mesin dan manusia menggunakan bahasa natural.\n",
    "\n",
    "Eksplorasi data merupakan langkah untuk memahami data sebelum dilakukan praproses. Pemahaman terhadap data yang akan di-mining dapat membantu dalam menentukan teknik-teknik pra-proses dan analisis data terhadap data sebelum dilakukan data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x-6qh4271XKT",
   "metadata": {
    "id": "x-6qh4271XKT"
   },
   "source": [
    "## **Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cde590",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47cde590",
    "outputId": "5cbdece0-4fb3-423f-f726-a0310737ef4d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "!pip install sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08253534",
   "metadata": {
    "id": "08253534"
   },
   "source": [
    "Import data\n",
    "silahkan gunakan data yang sudah kalian ambil pada hari selasa, data berupa text yang diambil di twitter dengan topik tertentu\n",
    "\n",
    "atau bisa gunakan data yang disediakan\n",
    "[disini](https://drive.google.com/file/d/10sIjKMFBhBlp5W4zCWFwXDars2c7koyR/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273fec20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "273fec20",
    "outputId": "5069a6d6-0bc3-4b68-87fa-b17f388956d1"
   },
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('/content/bjorka.csv')\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FaeMy4735SCR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "FaeMy4735SCR",
    "outputId": "86bedd0d-6a2c-4187-f332-abd3dc972ba3"
   },
   "outputs": [],
   "source": [
    "tweets = tweet.iloc[0:300,:]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758a873",
   "metadata": {
    "id": "7758a873"
   },
   "source": [
    "##Fungsi Preprocessing\n",
    "Text preprocessing adalah suatu proses untuk menyeleksi data text agar menjadi lebih terstruktur lagi dengan melalui serangkaian tahapan yang meliputi tahapan case folding, tokenizing, filtering dan stemming. Tapi, sesungguhnya tidak ada aturan pasti tentang setiap tahapan dalam text preprocessing. Semua itu tergantung dengan jenis serta kondisi data yang kita miliki. Text preprocessing merupakan salah satu implementasi dari text mining. Text mining sendiri adalah suatu kegiatan menambang data, dimana data yang biasanya diambil berupa text yang bersumber dari dokumen-dokumen yang memiliki goals untuk mencari kata kunci yang mewakili dari sekumpulan dokumen tersebut sehingga nantinya dapat dilakukan analisa hubungan antara dokumen-dokumen tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d90d7f",
   "metadata": {
    "id": "f6d90d7f"
   },
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # remove hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # remove RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # remove link\n",
    "    text = re.sub(r'[0-9]+', '', text) # remove numbers\n",
    "    text = re.sub(r'B|b', '', text)\n",
    "    text = re.sub(r'xexxa', '', text)\n",
    "\n",
    "    text = text.replace('\\n', ' ') # replace new line into space\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuations\n",
    "    text = text.strip(' ') # remove characters space from both left and right text\n",
    "    return text\n",
    "\n",
    "def casefoldingText(text): # Converting all the characters in a text into lower case\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def tokenizingText(text): # Tokenizing or splitting a string, text into a list of tokens\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def filteringText(text): # Remove stopwors in a text\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered\n",
    "    return text\n",
    "\n",
    "def stemmingText(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "def toSentence(list_words): # Convert list of words into sentence\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9a6b3",
   "metadata": {
    "id": "84d9a6b3"
   },
   "source": [
    "##Download Punkt\n",
    "Punkt dan stopword adalah corpus atau dataset yang disediakan oleh nltk yang akan banyak digunakan pada domain NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f873b90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f873b90",
    "outputId": "d856064d-b65b-4272-e552-4877cc6e8c62"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871d3b4",
   "metadata": {
    "id": "4871d3b4"
   },
   "source": [
    "##Proses Preprocessing,\n",
    "Tahap ini adalah menjalankan fungsi Preprocessing data kita\n",
    "\n",
    "sebagai gambaran 4000 data sekitar 1.5 jam sabar ya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77328c0d",
   "metadata": {
    "id": "77328c0d"
   },
   "outputs": [],
   "source": [
    "tweets['text_clean'] = tweets['isitweet'].apply(cleaningText)\n",
    "tweets['text_clean'] = tweets['text_clean'].apply(casefoldingText)\n",
    "tweets.drop(['isitweet'], axis = 1, inplace = True)\n",
    "\n",
    "tweets['text_preprocessed'] = tweets['text_clean'].apply(tokenizingText)\n",
    "tweets['text_preprocessed'] = tweets['text_preprocessed'].apply(filteringText)\n",
    "tweets['text_preprocessed'] = tweets['text_preprocessed'].apply(stemmingText)\n",
    "\n",
    "# drop duplicates/spams tweets\n",
    "tweets.drop_duplicates(subset = 'text_clean', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f7134",
   "metadata": {
    "id": "011f7134"
   },
   "source": [
    "## Menampilkan kalimat yang sudah di preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0r6hkN4wzXCs",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 919
    },
    "id": "0r6hkN4wzXCs",
    "outputId": "4533da37-8dbe-451c-8729-0f8a79f0210a"
   },
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OD6a5Apm6f-6",
   "metadata": {
    "id": "OD6a5Apm6f-6"
   },
   "source": [
    "## Labeling dengan lexicon\n",
    "Pelabelan data adalah proses penambahan label ke data mentah untuk memberikan konteks atau makna pada data. Data berlabel kemudian digunakan untuk melatih model NLP untuk membuat prediksi atau memahami atau menghasilkan ucapan.\n",
    "\n",
    "pelabelan kali ini menggunakan lexicon sebagai pelabelan secara otomatis. Dalam klasifikasi berbasis leksikon, dokumen diberi label dengan membandingkan jumlah kata yang muncul dari dua leksikon yang berlawanan, seperti sentimen positif dan negatif.\n",
    "\n",
    "Sebelum menjalankan kode berikut silahkan download 2 file pendukung berikut:\n",
    "\n",
    "[Negatif](https://drive.google.com/file/d/1UBLrxmfFTEg1t9rHw6v-M5pJVRDWuyFH/view?usp=sharing)\n",
    "\n",
    "[Positif](https://drive.google.com/file/d/1JqpJ12JqiOMOJ5R0Gg4rz3Xbqe3j2-Om/view?usp=sharing)\n",
    "\n",
    "[Referensi](https://github.com/fajri91/InSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be945df",
   "metadata": {
    "id": "0be945df"
   },
   "outputs": [],
   "source": [
    "# Loads lexicon positive and negative data\n",
    "lexicon_positive = dict()\n",
    "import csv\n",
    "with open('positive.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        lexicon_positive[row[0]] = int(row[1])\n",
    "\n",
    "lexicon_negative = dict()\n",
    "import csv\n",
    "with open('negative.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        lexicon_negative[row[0]] = int(row[1])\n",
    "\n",
    "# Function to determine sentiment polarity of tweets\n",
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    #for word in text:\n",
    "    score = 0\n",
    "    for word in text:\n",
    "        if (word in lexicon_positive):\n",
    "            score = score + lexicon_positive[word]\n",
    "    for word in text:\n",
    "        if (word in lexicon_negative):\n",
    "            score = score + lexicon_negative[word]\n",
    "    polarity=''\n",
    "    if (score > 0):\n",
    "        polarity = 'positive'\n",
    "    elif (score < 0):\n",
    "        polarity = 'negative'\n",
    "    else:\n",
    "        polarity = 'neutral'\n",
    "    return score, polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85d7c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea85d7c4",
    "outputId": "08cf75d0-6d66-43e2-a663-3c5adcacec5e"
   },
   "outputs": [],
   "source": [
    "results = tweets['text_preprocessed'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "results = list(zip(*results))\n",
    "tweets['polarity_score'] = results[0]\n",
    "tweets['polarity'] = results[1]\n",
    "print(tweets['polarity'].value_counts())\n",
    "\n",
    "# Export to csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XbOTesf775QB",
   "metadata": {
    "id": "XbOTesf775QB"
   },
   "source": [
    "## Visualisasi Hasil sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc537e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "3acc537e",
    "outputId": "b37bb1b4-b57a-402d-b24b-20a0e846c951"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "sizes = [count for count in tweets['polarity'].value_counts()]\n",
    "labels = list(tweets['polarity'].value_counts().index)\n",
    "explode = (0.1, 0, 0)\n",
    "ax.pie(x = sizes, labels = labels, autopct = '%1.1f%%', explode = explode, textprops={'fontsize': 14})\n",
    "ax.set_title(f'Sentiment Polarity on Tweets Data \\n (total = {len(tweets.index)} tweets)', fontsize = 16, pad = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8902f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "51a8902f",
    "outputId": "1eee8fe4-ec4d-4325-833c-0ad278378250"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 3000)\n",
    "positive_tweets = tweets[tweets['polarity'] == 'positive']\n",
    "positive_tweets = positive_tweets[['text_clean', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=False).reset_index(drop = True)\n",
    "positive_tweets.index += 1\n",
    "positive_tweets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0accf7e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "0accf7e2",
    "outputId": "f521dea7-e250-4a88-c9d2-017dcfcf9b9f"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 3000)\n",
    "negative_tweets = tweets[tweets['polarity'] == 'negative']\n",
    "negative_tweets = negative_tweets[['text_clean', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=True)[0:10].reset_index(drop = True)\n",
    "negative_tweets.index += 1\n",
    "negative_tweets[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gv4MW0e97p8C",
   "metadata": {
    "id": "gv4MW0e97p8C"
   },
   "source": [
    "##Visualize word cloud\n",
    "Word cloud merupakan salah satu metode untuk menampilkan data teks secara visual. Grafik ini populer dalam text mining karena mudah dipahami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f50143",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "60f50143",
    "outputId": "20867b89-2e3b-4d2f-838e-b86d0b86b729"
   },
   "outputs": [],
   "source": [
    "# Visualize word cloud\n",
    "\n",
    "list_words=''\n",
    "for tweet in tweets['text_preprocessed']:\n",
    "    for word in tweet:\n",
    "        list_words += ' '+(word)\n",
    "\n",
    "wordcloud = WordCloud(width = 600, height = 400, background_color = 'black', min_font_size = 10).generate(list_words)\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "ax.set_title('Word Cloud of Tweets Data', fontsize = 18)\n",
    "ax.grid(False)\n",
    "ax.imshow((wordcloud))\n",
    "fig.tight_layout(pad=0)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LWhYw6i98BXh",
   "metadata": {
    "id": "LWhYw6i98BXh"
   },
   "source": [
    "#Tugas\n",
    "Silahkan kalian labeling data yang sudah di crawling pada hari ke 2 kemarin sebagai mini project!\n",
    "\n",
    "kemudian simpan hasilnya dalam csv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iBHfyDwiVy5O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBHfyDwiVy5O",
    "outputId": "63582b00-91b5-484a-eea0-1a6602415df5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the recipe data\n",
    "data = {\n",
    "    \"name\": [\"Nasi Liwet\", \"Karedok\", \"Lotek\"],\n",
    "    \"description\": [\n",
    "        \"Nasi liwet adalah nasi gurih khas Sunda yang dimasak dengan santan dan rempah, sering disajikan dengan ikan asin, ayam, dan sayuran.\",\n",
    "        \"Karedok adalah salad sayuran khas Sunda dengan bumbu kacang yang segar dan pedas.\",\n",
    "        \"Lotek adalah hidangan khas Sunda yang mirip dengan gado-gado, terdiri dari sayuran rebus yang disajikan dengan bumbu kacang.\"\n",
    "    ],\n",
    "    \"ingredients\": [\n",
    "        \"300 gram beras, 400 ml air, 200 ml santan, 1 batang serai, 2 lembar daun salam, 2 lembar daun jeruk, 1 sendok teh garam\",\n",
    "        \"100 gram kacang panjang, 100 gram tauge, 50 gram kol, 1 buah mentimun, 5 lembar daun kemangi\",\n",
    "        \"100 gram kacang panjang, 100 gram tauge, 50 gram kol, 100 gram bayam, 1 buah ketimun\"\n",
    "    ],\n",
    "    \"steps\": [\n",
    "        \"Cuci beras hingga bersih. Masukkan beras, air, santan, serai, daun salam, daun jeruk, dan garam ke dalam rice cooker atau panci. Masak hingga nasi matang dan bumbu meresap. Setelah matang, angkat dan sajikan nasi liwet dengan lauk pendamping.\",\n",
    "        \"Siapkan sayuran, cuci bersih dan tiriskan. Haluskan bumbu kacang, lalu campur dengan sayuran. Aduk rata dan sajikan.\",\n",
    "        \"Rebus semua sayuran hingga setengah matang, lalu tiriskan. Haluskan bumbu kacang, campurkan dengan sayuran rebus. Sajikan lotek dengan tambahan lontong atau ketupat jika diinginkan.\"\n",
    "    ],\n",
    "    \"bumbu_kacang\": [\n",
    "        None,\n",
    "        \"100 gram kacang tanah, 3 siung bawang putih, 5 buah cabai rawit merah, 2 cm kencur, 1 sendok teh gula merah, 1 sendok teh garam, 100 ml air matang\",\n",
    "        \"100 gram kacang tanah, 3 siung bawang putih, 5 buah cabai merah, 1 sendok teh gula merah, 1 sendok teh garam, 100 ml air matang\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame as CSV\n",
    "df.to_csv('recipe_data.csv', index=False)\n",
    "print(\"DataFrame saved to recipe_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
